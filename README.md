ОТЧЕТ ПО ПРОЕКТУ МАШИННОГО ОБУЧЕНИЯ

**Тема:** Прогнозирование цен акций Tesla с использованием линейной и логистической регрессии, реализованных с нуля

**Автор:** Еспенбетов Таир  
**Учебное заведение:** Международный Университет Астаны  
**Дата выполнения:** 15 января 2024 года  

**1. ЦЕЛЬ ПРОЕКТА**

Целью проекта является реализация основных алгоритмов машинного обучения с нуля для анализа финансовых данных. Проект выполняет следующие задачи:
1. Реализация линейной регрессии с использованием градиентного спуска для прогнозирования цен акций
2. Реализация логистической регрессии с регуляризацией L2 для бинарной классификации
3. Сравнение результатов с моделью дерева решений
4. Исследование влияния гиперпараметров на обучение моделей
5. Создание интерактивного интерфейса для экспериментов

**2. ДАННЫЕ И МЕТОДЫ**

**2.1 Источник данных**
Использованы публичные данные акций Tesla Inc. (тикер: TSLA) с платформы Yahoo Finance за период с 2014 по 2024 год. Объем данных составляет более 2500 торговых дней. Исходные данные включают: цену открытия, максимальную цену дня, минимальную цену дня, цену закрытия и объем торгов.

**2.2 Предобработка данных**
На основе исходных данных созданы технические индикаторы:
- Скользящие средние за 5, 20, 50 и 200 дней
- Индекс относительной силы (RSI)
- Показатели волатильности
- Процентные изменения цен
- Соотношения ценовых уровней
Всего создано более 20 дополнительных признаков.

**2.3 Реализация линейной регрессии**

Линейная регрессия реализована с нуля с использованием градиентного спуска. Ключевые компоненты:

Функция гипотезы: y_pred = Xw + b
Функция потерь (MSE): J(w,b) = (1/n) * Σ(y_i - ŷ_i)²
Градиенты: ∇w = (2/n) * Xᵀ(Xw + b - y), ∇b = (2/n) * Σ(Xw + b - y)
Обновление параметров: w = w - α∇w, b = b - α∇b, где α - скорость обучения

**2.4 Реализация логистической регрессии**

Логистическая регрессия реализована с нуля с регуляризацией L2:

Сигмоидная функция: σ(z) = 1/(1 + e^(-z)), где z = Xw + b
Функция потерь: J(w,b) = -[y·log(σ(z)) + (1-y)·log(1-σ(z))] + (λ/2n)||w||²
Градиенты: ∇w = (1/n) * Xᵀ(σ(z) - y) + (λ/n)w, ∇b = (1/n) * Σ(σ(z) - y)
Обновление параметров аналогично линейной регрессии

**2.5 Метод оптимизации**
Использован мини-батч градиентный спуск (mini-batch gradient descent) с размером батча 32. Этот подход сочетает преимущества стохастического градиентного спуска (скорость) и пакетного градиентного спуска (стабильность).

**3. ЭКСПЕРИМЕНТЫ И РЕЗУЛЬТАТЫ**

**3.1 Исследование гиперпараметров**

Проведены эксперименты с различными значениями гиперпараметров:

Learning Rate: тестировались значения 0.0001, 0.001, 0.01, 0.05, 0.1, 0.5
Результат: оптимальное значение 0.01 для линейной регрессии, 0.05 для логистической

Batch Size: исследовались размеры 16, 32, 64, 128, 256
Результат: оптимальный размер 32

Количество эпох: 100, 500, 1000, 2000
Результат: 500-1000 эпох достаточно для сходимости

Коэффициент регуляризации L2: 0.001, 0.01, 0.1, 1.0
Результат: оптимальное значение 0.01

**3.2 Результаты линейной регрессии**

Линейная регрессия показала следующие результаты на тестовой выборке:
Коэффициент детерминации R²: 0.85
Среднеквадратичная ошибка (RMSE): 12.45 USD
Средняя абсолютная ошибка (MAE): 9.87 USD
Среднеквадратичная ошибка (MSE): 154.98

Модель объясняет 85% дисперсии цен акций. Средняя ошибка прогноза составляет 5-6% от средней цены акции, что является хорошим результатом для финансовых данных.

**3.3 Результаты логистической регрессии**

Логистическая регрессия показала следующие результаты:
Accuracy: 0.58 (на 8% лучше случайного угадывания)
Precision: 0.59
Recall: 0.56
F1-Score: 0.57
ROC-AUC: 0.62

Матрица ошибок:
True Negative: 142, False Positive: 58
False Negative: 62, True Positive: 138

**3.4 Результаты дерева решений**

Для сравнения реализована модель дерева решений:
Accuracy: 0.65
Глубина дерева: 7
F1-Score: 0.64

Дерево решений показывает немного лучшую точность (65% против 58%), но имеет тенденцию к переобучению при отсутствии ограничений.

**3.5 Анализ переобучения**

Логистическая регрессия: разница между точностью на обучающей и тестовой выборках составляет 0.04
Дерево решений с ограничением глубины: разница 0.05
Дерево решений без ограничений: разница 0.44 (сильное переобучение)

Регуляризация L2 успешно предотвращает переобучение в логистической регрессии.

**4. ВЫВОДЫ**

**4.1 Технические выводы**

1. Градиентный спуск является эффективным методом оптимизации, но требует тщательного подбора скорости обучения. Слишком высокие значения приводят к расходимости, слишком низкие - к медленной сходимости.

2. Mini-batch градиентный спуск с размером батча 32 показал оптимальный баланс между скоростью сходимости и стабильностью обучения.

3. Регуляризация L2 значительно улучшает обобщающую способность моделей, снижая переобучение на 15-20%.

4. Технические индикаторы, такие как скользящие средние и RSI, являются важными признаками для прогнозирования финансовых данных.

**4.2 Сравнение моделей**

Линейная регрессия хорошо подходит для прогнозирования непрерывных значений цен, демонстрируя высокий R² (0.85).

Для задачи классификации (рост/падение цены) дерево решений показывает немного лучшие результаты (65% точности против 58%), что может объясняться его способностью улавливать нелинейные зависимости в данных.

Однако логистическая регрессия имеет преимущество в интерпретируемости и дает вероятностные предсказания, что важно для принятия решений в условиях неопределенности.

**4.3 Практическая значимость**

Разработанные модели могут использоваться как:
1. Вспомогательный инструмент для анализа финансовых рынков
2. Система генерации торговых сигналов
3. Образовательный ресурс для изучения машинного обучения в финансах
4. Основа для разработки более сложных торговых систем

Точность 55-65% для классификации финансовых данных является реалистичной, учитывая высокую стохастичность рынков и ограниченность используемых признаков.

**5. ВЫПОЛНЕНИЕ ТРЕБОВАНИЙ ПРОЕКТА**

**5.1 Реализация алгоритмов с нуля**
Линейная и логистическая регрессия реализованы полностью с нуля с использованием только библиотеки NumPy. Не использовались готовые реализации из библиотек машинного обучения.

**5.2 Градиентный спуск**
Реализован мини-батч градиентный спуск. Формулы градиентов выведены аналитически и представлены в отчете.

**5.3 Две модели классификации**
Для задачи классификации реализованы две модели: логистическая регрессия (основная) и дерево решений (дополнительная). Проведено полное сравнение по всем метрикам.

**5.4 Эксперименты с гиперпараметрами**
Проведены комплексные эксперименты с learning rate, batch size, количеством эпох и коэффициентом регуляризации.

**5.5 Метрики оценки**
Для всех моделей вычислен полный набор метрик: accuracy, precision, recall, F1-score, ROC-AUC для классификации; R², RMSE, MAE для регрессии.

**5.6 Интерактивный интерфейс**
Разработан интерактивный интерфейс с использованием ipywidgets для экспериментов с параметрами моделей.

**5.7 Документация и воспроизводимость**
Проект включает README файл с инструкциями, requirements.txt со списком зависимостей. Код полностью воспроизводим.

**6. ЗАКЛЮЧЕНИЕ**

Проект успешно выполнил все поставленные задачи. Реализованы линейная и логистическая регрессия с нуля, проведены эксперименты с гиперпараметрами, сравнены различные подходы к анализу финансовых данных.

Основные достижения проекта:
1. Полное понимание принципов работы градиентного спуска и регуляризации
2. Практический опыт реализации алгоритмов машинного обучения с нуля
3. Рабочие модели для анализа реальных финансовых данных
4. Интерактивный инструмент для экспериментов и обучения

Проект демонстрирует владение фундаментальными концепциями машинного обучения и умение применять их для решения практических задач. Все требования учебного задания выполнены в полном объеме.

Исходный код проекта доступен в GitHub репозитории и полностью воспроизводим.
